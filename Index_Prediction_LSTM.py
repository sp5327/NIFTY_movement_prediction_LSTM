# -*- coding: utf-8 -*-
"""Untitled12.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18v4HV5xGwzi5gCw2oefMp0xSRSf9ap2t
"""

pip install yfinance

import yfinance as yf
data = yf.download('^NSEI','2009-01-01','2023-12-19')

print(data.head())

import pandas as pd
import keras as ks
import numpy as np
df = pd.DataFrame(data)

print(df.head())
print(df.tail())

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
scaled_data = scaler.fit_transform(df)
scaled_df = pd.DataFrame(scaled_data, columns=df.columns)
print(df)
print(scaled_df)

import matplotlib.pyplot as plt
plt.plot(df['Close'])
plt.show()
plt.plot(scaled_df['Close'])
plt.show()

"""

The ACF and PACF plots show that there is some significant autocorrelation in the data, with the first few lags being the most significant. This suggests that the data may be non-stationary, and that a differencing transformation may be necessary to make it stationary.

The PACF plot shows that there is also some significant partial autocorrelation, with the first few lags being the most significant. This suggests that the data may have some serial correlation, and that an ARIMA model may be appropriate for forecasting.

In order to determine the best model for forecasting the data, it would be necessary to perform further analysis, such as fitting different ARIMA models and evaluating their performance on a holdout test set."""

from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

plot_acf(scaled_df['Close'], lags=20)
plt.show()
plot_pacf(scaled_df['Close'], lags=20)
plt.show()

"""Fixed size window"""

# Function to create sequences from the time series data
def create_sequences(data, window_size, stride):
    sequences = []
    targets = []
    for i in range(0, len(data) - window_size , stride):
        sequence = data.iloc[i:i+window_size]['Close'].values
        target = data.iloc[i+window_size]['Close']
        sequences.append(sequence)
        targets.append(target)
    return np.array(sequences), np.array(targets)

# Set the window size and stride
window_size = 5
stride = 1

# Create sequences and targets
sequences, targets = create_sequences(scaled_df, window_size, stride)

# Reshape sequences for compatibility with train_test_split
sequences = sequences.reshape(-1, window_size)

# Split the data into training, validation, and test sets
# For simplicity, let's use an 80-10-10 split
X_train, X_temp, y_train, y_temp = train_test_split(sequences, targets, test_size=0.2, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Display the shapes of the resulting sets
print("X_train shape:", X_train.shape)
print("X_val shape:", X_val.shape)
print("X_test shape:", X_test.shape)
print("y_train shape:", y_train.shape)
print("y_val shape:", y_val.shape)
print("y_test shape:", y_test.shape)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from tensorflow.keras.optimizers import Adam
from keras.layers import Dropout

X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))
X_val = X_val.reshape((X_val.shape[0], X_val.shape[1], 1))
X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))

print(X_train.shape)
print(X_val.shape)
print(X_test.shape)

"""Building the LSTM model"""

model = Sequential()

# First LSTM layer
model.add(LSTM(units=50, return_sequences=True, activation='relu', input_shape=(window_size, 1)))
model.add(Dropout(0.2))

# Second LSTM layer
model.add(LSTM(units=32, return_sequences=True))
model.add(Dropout(0.2))

# Third LSTM layer
model.add(LSTM(units=32, return_sequences=True))
model.add(Dropout(0.2))

# Fourth LSTM layer
model.add(LSTM(units=32, return_sequences=True))
model.add(Dropout(0.2))

# Fifth LSTM layer
model.add(LSTM(units=32))
model.add(Dropout(0.2))

# Output layer
model.add(Dense(units=1, activation="linear"))

model.summary()

model.compile(loss="mse", optimizer="adam", metrics=["mae"])


history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_val, y_val))


loss = model.evaluate(X_test, y_test)
print(f'Test Loss: {loss}')

# Print keys in history.history
keys = list(history.history.keys())
print("Available keys in history.history:", keys)

import matplotlib.pyplot as plt
predictions = model.predict(X_test)
plt.plot(y_test, label="Actual")
plt.plot(predictions, label="Predicted")
plt.xlabel("Time")
plt.ylabel("Closing Price")
plt.legend()
plt.show()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

plt.plot(history.history['mae'])
plt.plot(history.history['val_mae'])
plt.title('Model mean absolute error')
plt.ylabel('MAE')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

# Residuals
residuals = y_test - predictions
plt.plot(residuals)
plt.title('Residuals')
plt.xlabel('Time')
plt.ylabel('Residuals')
plt.show()

# Histogram of residuals
plt.hist(residuals)
plt.title('Histogram of residuals')
plt.xlabel('Residuals')
plt.ylabel('Frequency')
plt.show()

from keras.utils import plot_model
plot_model(model, show_shapes=True, show_layer_names=True)

# Replace X_new with your actual new dataset

X_new = yf.download('^NSEI','2023-01-01','2023-12-19')

scaler1 = MinMaxScaler()
scaled_data1 = scaler1.fit_transform(X_new)
scaled_df1 = pd.DataFrame(scaled_data1, columns=X_new.columns)

scaled_df1 = scaled_df1.to_numpy()

# Reshape X_new for LSTM input
scaled_df1 = scaled_df1.reshape((scaled_df1.shape[0], scaled_df1.shape[1], 1))

# Make predictions using the trained model
new_predictions = model.predict(scaled_df1)




print("Predictions:")
print(new_predictions)

"""what above predictions tells

The predictions represent the estimated closing prices of the NSEI for the dates specified in the X_new dataset.
The predictions can be used to make informed decisions about investment or trading strategies.
For example, if a prediction is higher than the current closing price, it may indicate a potential buying opportunity.
Conversely, if a prediction is lower than the current closing price, it may indicate a potential selling opportunity.
However, it is important to note that these predictions are based on historical data and are not guaranteed to be accurate.
It is always advisable to conduct further analysis and consider other factors before making any investment or trading decisions.
"""

# visualise the predicted new data on the model with the actual data

plt.plot(y_test, label="Actual")
plt.plot(new_predictions, label="Predicted")
plt.xlabel("Time")
plt.ylabel("Closing Price")
plt.legend()
plt.show()

#  how much acurate is my model in terms of percentage

mae = loss[1]
accuracy = 100 - (mae / y_test.mean() * 100)
print(f'Accuracy: {accuracy:.2f}%')